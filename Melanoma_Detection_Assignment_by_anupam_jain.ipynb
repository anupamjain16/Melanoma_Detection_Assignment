{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define paths for the dataset\n",
        "train_dir = '/content/train'\n",
        "test_dir = '/content/test'\n",
        "\n",
        "# Constants\n",
        "IMG_HEIGHT, IMG_WIDTH = 180, 180\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "NUM_CLASSES = 9\n",
        "\n",
        "# Step 1: Dataset Creation and Preprocessing\n",
        "# Create training and validation datasets\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_dataset = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_dataset = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Step 2: Visualize one instance of each class\n",
        "def visualize_classes(dataset):\n",
        "    class_names = list(dataset.class_indices.keys())\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "    axes = axes.flatten()\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        for x, y in dataset:\n",
        "            if y[0][i] == 1:\n",
        "                axes[i].imshow(x[0])\n",
        "                axes[i].set_title(class_name)\n",
        "                axes[i].axis('off')\n",
        "                break\n",
        "    plt.show()\n",
        "\n",
        "visualize_classes(train_dataset)\n",
        "\n",
        "# Step 3: Model Building\n",
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n",
        "\n",
        "# Step 4: Model Training\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset\n",
        ")\n",
        "\n",
        "# Step 5: Plot training results\n",
        "def plot_training(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range = range(EPOCHS)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "plot_training(history)\n",
        "\n",
        "# Analyze results for signs of overfitting or underfitting\n",
        "# If there is overfitting, we will move to data augmentation next\n",
        "\n",
        "# Step 6: Data Augmentation to Reduce Overfitting\n",
        "train_datagen_augmented = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_dataset_augmented = train_datagen_augmented.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_dataset_augmented = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Re-train the model with augmented data\n",
        "model_augmented = create_model()\n",
        "history_augmented = model_augmented.fit(\n",
        "    train_dataset_augmented,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset_augmented\n",
        ")\n",
        "\n",
        "# Plot training results for augmented data\n",
        "plot_training(history_augmented)\n",
        "\n",
        "# Step 7: Class Distribution and Handling Class Imbalance\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "class_counts = Counter(train_dataset_augmented.classes)\n",
        "class_distribution = pd.DataFrame.from_dict(class_counts, orient='index', columns=['Count'])\n",
        "print(\"Class Distribution:\")\n",
        "print(class_distribution)\n",
        "\n",
        "# Find classes with fewer samples and apply class augmentation using Augmentor if necessary\n",
        "# Example for handling class imbalance using the `Augmentor` library\n",
        "!pip install Augmentor\n",
        "import Augmentor\n",
        "\n",
        "# Path setup for class augmentation (example for one class)\n",
        "augmentor_pipeline = Augmentor.Pipeline(\"/path/to/class_with_least_samples\")\n",
        "augmentor_pipeline.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "augmentor_pipeline.zoom_random(probability=0.5, percentage_area=0.8)\n",
        "augmentor_pipeline.sample(500)  # Generate 500 samples to balance classes\n",
        "\n",
        "# After balancing, retrain the model\n",
        "model_balanced = create_model()\n",
        "history_balanced = model_balanced.fit(\n",
        "    train_dataset_augmented,\n",
        "    epochs=30,\n",
        "    validation_data=val_dataset_augmented\n",
        ")\n",
        "\n",
        "# Plot training results for balanced dataset\n",
        "plot_training(history_balanced)\n"
      ],
      "metadata": {
        "id": "RDsoEEYSed8k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}